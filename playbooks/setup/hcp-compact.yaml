---
# This playbook expects a ctlplane lab with extra memory.
# MGMT-17117
- name: 'Install Hypershift cluster with HCPs running on management control plane'
  hosts:
    - 'lab'

  gather_facts: false

  vars:
    lab_dnsmasq_root: '/opt/dnsmasq'
    lab_name: 'hypershift'
    lab_configs: "/root/labs/{{ lab_name }}/config"
    lab_mac_base: 'be:be:ca:fe:02:'
    lab_network_name: 'lab-network'
    lab_node_network: '192.168.125'
    lab_install: true
    ocp_metallb_pool_ips: "{{ lab_node_network }}.40-{{ lab_node_network }}.45"
    ocp_metallb_pool_name: "{{ lab_network_name }}"
    ocp_baremetal_infras:
      - name: 'hcp1'
        ns: 'hardware-inventory'
        hypershift: true
        pullsecret: "{{ lab_pull_secret }}"
        sshkey: "{{ lab_ssh_pubkey }}"
        ntp: ["{{ lab_node_network }}.1"]
        redfish: "{{ lab_node_network }}.1"
        inventory:
          - {'name': 'hcp1-worker-1', 'id': '51'}
      - name: 'hcp2'
        ns: 'hardware-inventory'
        hypershift: true
        pullsecret: "{{ lab_pull_secret }}"
        sshkey: "{{ lab_ssh_pubkey }}"
        ntp: ["{{ lab_node_network }}.1"]
        redfish: "{{ lab_node_network }}.1"
        inventory:
          - {'name': 'hcp2-worker-1', 'id': '61'}
    ocp_hcp_clusters:
      - name: 'hcp1'
        ns: 'hcp1'
        ctl_availability: 'HighlyAvailable'  # Cluster controllers (etcd, kube-apiserver, ...)
        infra: 'hcp1'
        agent_ns: 'hardware-inventory'
        pullsecret: "{{ lab_pull_secret_b64 }}"
        sshkey: "{{ lab_ssh_pubkey }}"
        version: '4.14.8'
        service_network: '172.32.0.0/16'
        domain: 'local.lab'
        lb: "{{ lab_node_network }}.50"
        replicas: 1  # In this scenario we need at least 1 worker to start the installation
      - name: 'hcp2'
        ns: 'hcp2'
        ctl_availability: 'HighlyAvailable'  # Cluster controllers (etcd, kube-apiserver, ...)
        infra: 'hcp2'
        agent_ns: 'hardware-inventory'
        pullsecret: "{{ lab_pull_secret_b64 }}"
        sshkey: "{{ lab_ssh_pubkey }}"
        version: '4.14.13'
        service_network: '172.33.0.0/16'
        domain: 'local.lab'
        lb: "{{ lab_node_network }}.60"
        replicas: 1  # In this scenario we need at least 1 worker to start the installation

  environment:
    KUBECONFIG: "/root/labs/{{ lab_name }}/deploy/auth/kubeconfig"

  pre_tasks:
    - name: 'Set different hypershift zones in nodes to ensure balanced distribution in HCP pods'
      ansible.builtin.command:
        cmd: >
          /usr/local/bin/oc label node {{ item }} topology.kubernetes.io/zone={{ item }}
      loop:
        - "{{ lab_name }}-master-1"
        - "{{ lab_name }}-master-2"
        - "{{ lab_name }}-master-3"

    - name: 'Set hypershift control plane label'
      ansible.builtin.command:
        cmd: >
          /usr/local/bin/oc label node {{ item }} hypershift.openshift.io/control-plane=true
      loop:
        - "{{ lab_name }}-master-1"
        - "{{ lab_name }}-master-2"
        - "{{ lab_name }}-master-3"

  roles:
    - 'ocp_hcp'
